import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "OpenBuddy/openbuddy-llama2-13b-v8.1-fp16"
model = AutoModelForCausalLM.from_pretrained(
    model_path, device_map="auto", trust_remote_code=True, torch_dtype=torch.float16
)
tokenizer = AutoTokenizer.from_pretrained(model_path)

model.eval()

with open(
    "/cluster/home/repo/my_llm_experiments/esrs_data_collection/srn_data_collector/system.prompt", "r", encoding="utf-8"
) as f:
    prompt = f.read()

# prompt += "\n\nUser: Can zou translate text from english to german ?\nAssistant:"

prompt += """\n\nUser: translate the below text, to German language :
Due to the broader definition of Scope 1 and Scope 2 emissions generated by BMW Group locations in the year under report and adjustments to the methodology for calculating use-phase emissions, the years 2019 (base year) and 2020 have been adjusted for comparison purposes. For these reasons, a direct comparison with 
2017 and 2018 figures is not possible.
Assistant:
"""

input_ids = tokenizer.encode(prompt, return_tensors="pt").to("cuda")

with torch.no_grad():
    output_ids = model.generate(input_ids=input_ids, max_new_tokens=2000, eos_token_id=tokenizer.eos_token_id)

print(tokenizer.decode(output_ids[0], skip_special_tokens=True))
