{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21169e0d-aa7e-4351-af4e-03b0a621e1f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/langchain_tut/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 727/727 [00:00<00:00, 1.15MB/s]\n",
      "tokenizer.model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 42.1MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 30.5MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 638kB/s]\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 837/837 [00:00<00:00, 1.35MB/s]\n",
      "model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.26G/7.26G [03:55<00:00, 30.9MB/s]\n",
      "generation_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 132/132 [00:00<00:00, 212kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    " \n",
    "MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
    ")\n",
    " \n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.0001\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    " \n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    " \n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0224ba06-d2f8-41b1-800e-0ca6a8c920ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: Sure! Here's the difference between ChatGPT and open-source large language models (LLMs) in two lines:\n",
      "\n",
      "ChatGPT is a proprietary, closed-source AI model developed by Meta AI that offers a more user-friendly interface and seamless integration with other Meta products, while open-source LLMs like BERT and RoBERTa are freely available for anyone to use and modify, but may require more technical expertise to integrate into applications.\n"
     ]
    }
   ],
   "source": [
    "result = llm(\n",
    "    \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c328a55-944f-4ee6-834b-00052fb33bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    " \n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Act as a Machine Learning engineer who is teaching high school students.\n",
    "<</SYS>>\n",
    " \n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da828836-19c4-4d5a-b724-b78d650f2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] <<SYS>>\n",
      "Act as a Machine Learning engineer who is teaching high school students.\n",
      "<</SYS>>\n",
      " \n",
      "Explain what are Deep Neural Networks in 2-3 sentences [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Explain what are Deep Neural Networks in 2-3 sentences\"\n",
    "print(prompt.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b0d82c-8e22-4946-9ed8-d562db7fefe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey there, young minds! So, you wanna know about Deep Neural Networks? Well, imagine you have a super powerful computer that can learn and make decisions all on its own, kinda like how your brain works! Deep Neural Networks are like a bunch of these computers working together to solve really tough problems, like recognizing pictures or understanding speech. They're like the ultimate team players, and they're changing the game in fields like self-driving cars, medical diagnosis, and more!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    " \n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "result = chain.run(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38cdaed9-d25d-4f47-a2e1-a1b392525a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"<s>[INST] Use the summary {summary} and give 3 examples of practical applications with 1 sentence explaining each [/INST]\"\n",
    " \n",
    "examples_prompt = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=template,\n",
    ")\n",
    "examples_chain = LLMChain(llm=llm, prompt=examples_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c95ad77e-7524-4637-9123-7e15f152ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Hey there, young minds! So, you wanna know about Deep Neural Networks? Well, imagine you have a super powerful computer that can learn and make decisions all on its own, kinda like how your brain works! Deep Neural Networks are like a bunch of these computers working together to solve really tough problems, like recognizing pictures or understanding speech. They're like the ultimate team players, and they're changing the game in fields like self-driving cars, medical diagnosis, and more!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m  Sure thing! Here are three examples of practical applications of Deep Neural Networks:\n",
      "\n",
      "1. Self-Driving Cars: Deep Neural Networks can be used to train autonomous vehicles to recognize objects on the road, such as pedestrians, other cars, and traffic lights, allowing them to make safe and efficient decisions.\n",
      "2. Medical Diagnosis: Deep Neural Networks can be trained on large datasets of medical images and patient data to help doctors diagnose diseases and conditions more accurately and efficiently than ever before.\n",
      "3. Speech Recognition: Deep Neural Networks can be used to improve speech recognition systems, enabling devices like smartphones and virtual assistants to better understand and respond to voice commands.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Sure thing! Here are three examples of practical applications of Deep Neural Networks:\n",
      "\n",
      "1. Self-Driving Cars: Deep Neural Networks can be used to train autonomous vehicles to recognize objects on the road, such as pedestrians, other cars, and traffic lights, allowing them to make safe and efficient decisions.\n",
      "2. Medical Diagnosis: Deep Neural Networks can be trained on large datasets of medical images and patient data to help doctors diagnose diseases and conditions more accurately and efficiently than ever before.\n",
      "3. Speech Recognition: Deep Neural Networks can be used to improve speech recognition systems, enabling devices like smartphones and virtual assistants to better understand and respond to voice commands.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    " \n",
    "multi_chain = SimpleSequentialChain(chains=[chain, examples_chain], verbose=True)\n",
    "result = multi_chain.run(text)\n",
    "print(result.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7213b184-ad7a-4fe4-84e7-cffcbad9580e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    " \n",
    "loader = UnstructuredMarkdownLoader(\"bitcoin.md\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3f61c32-c21b-4e21-b0b1-86c1b61c313f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    " \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1465cde2-3d5d-4f1a-8f79-c9befda821d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".gitattributes: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 2.20MB/s]\n",
      "1_Pooling/config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 191/191 [00:00<00:00, 325kB/s]\n",
      "README.md: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67.9k/67.9k [00:00<00:00, 781kB/s]\n",
      "config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 619/619 [00:00<00:00, 986kB/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 670M/670M [00:16<00:00, 39.5MB/s]\n",
      "onnx/config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 632/632 [00:00<00:00, 1.04MB/s]\n",
      "model.onnx: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.34G/1.34G [00:33<00:00, 39.3MB/s]\n",
      "onnx/special_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 205kB/s]\n",
      "onnx/tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712k/712k [00:00<00:00, 7.72MB/s]\n",
      "onnx/tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 342/342 [00:00<00:00, 728kB/s]\n",
      "onnx/vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 1.28MB/s]\n",
      "pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 670M/670M [00:18<00:00, 35.9MB/s]\n",
      "sentence_bert_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57.0/57.0 [00:00<00:00, 151kB/s]\n",
      "special_tokens_map.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 364kB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712k/712k [00:00<00:00, 7.53MB/s]\n",
      "tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 342/342 [00:00<00:00, 578kB/s]\n",
      "vocab.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 74.8MB/s]\n",
      "modules.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 385/385 [00:00<00:00, 992kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    " \n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    " \n",
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e53119fe-42fb-49cb-8956-9b54223d2aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we implement the proof-of-work by\\nincrementing a nonce in the block until a value is found that gives the\\nblock's hash the required zero bits. Once the CPU effort has been\\nexpended to make it satisfy the proof-of-work, the block cannot be\\nchanged without redoing the work. As later blocks are chained after it,\\nthe work to change the block would include redoing all the blocks after\\nit\\n\\nThe proof-of-work also solves the problem of determining representation\\nin majority decision making. If the majority were based on\\none-IP-address-one-vote, it could be subverted by anyone able to\\nallocate many IPs. Proof-of-work is essentially one-CPU-one-vote. The\\nmajority decision is represented by the longest chain, which has the\\ngreatest proof-of-work effort invested in it. If a majority of CPU power\\nis controlled by honest nodes, the honest chain will grow the fastest\\nand outpace any competing chains. To modify a past block, an attacker\\nwould have to redo the proof-of-work of the block and all blocks\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n",
    "results = db.similarity_search(\"proof-of-work majority decision making\", k=2)\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f475fcae-d3f1-4122-a601-dd055695cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, little buddy! So you know how there are lots of different people in the world, right? And sometimes, these people might not agree on things, like what game to play or what candy to eat. Well, imagine that instead of people, we have computers that talk to each other and do things together. These computers are like really smart and can do lots of things fast, like calculate numbers really quickly!\n",
      "\n",
      "Now, when these computers want to decide something, like which game to play or which candy to eat, they need to figure out who gets to decide. And that's where proof-of-work comes in! It's like a special trick that helps the computers decide who gets to say what.\n",
      "\n",
      "Here's how it works: when a computer wants to decide something, it has to show everyone else that it did some work first. Like, it has to solve a really hard math problem that takes a long time to solve. And once it solves that problem, it gets to say what it wants to say! But here's the cool part: if another computer comes along and tries to change what the first computer said, it has to solve the same hard math problem again! And if it can't solve it, then the first computer's idea is still the one that counts!\n",
      "\n",
      "So, proof-of-work is like a special badge that shows that a computer did some work and deserves to have its ideas heard. And because it's so hard to solve the math problems, it's like only the strongest and smartest computers can get these badges, and they're the ones that get to decide things! Isn't that cool?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    " \n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Act as a cryptocurrency expert. Use the following information to answer the question at the end.\n",
    "<</SYS>>\n",
    " \n",
    "{context}\n",
    " \n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    " \n",
    " \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    " \n",
    "result = qa_chain(\n",
    "    \"How does proof-of-work solves the majority decision making problem? Explain like I am five.\"\n",
    ")\n",
    "print(result[\"result\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c99b563-06e2-463f-a8f3-187d93b0671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an expert in cryptocurrency, I can explain that the privacy offered by\n",
      "Bitcoin's decentralized ledger technology is significantly better than the\n",
      "traditional banking model. In the traditional banking model, financial\n",
      "transactions are recorded on a centralized ledger controlled by a single entity,\n",
      "which can potentially compromise the privacy of users. In contrast, Bitcoin's\n",
      "decentralized ledger technology allows for the recording of transactions without\n",
      "revealing the identities of the parties involved, providing greater privacy and\n",
      "anonymity for users. Additionally, the use of public keys instead of real names\n",
      "further enhances the privacy features of the network.\n"
     ]
    }
   ],
   "source": [
    "from textwrap import fill\n",
    " \n",
    "result = qa_chain(\n",
    "    \"Summarize the privacy compared to the traditional banking model in 2-3 sentences.\"\n",
    ")\n",
    "print(fill(result[\"result\"].strip(), width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac393e64-fe01-4ede-b1d1-eac7b707ac3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb897f-dffb-451c-bd38-3d7a7fc33ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
